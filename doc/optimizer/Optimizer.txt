* Building & Running
  Steps to build and run our DAMAJ Decaf Compiler.
  
  These instructions assume you are running on Athena.
  
  Clone the repository or otherwise obtain a copy of the code.

  Run ~add -f scala~. use ~scala -version~ to make sure that scala is using version 2.11.2.
  We use ~fsc~ to compile scala a little faster. If you happen to have any trouble because of weird
  fsc version conflicts, please try killing your fsc server (find it with ~ps aux | grep fsc~).
  Don't set any env variables like ~SCALA_HOME~ as this could force you to use the wrong version
  of scala.
  
  Once you have the right version of scala run ~make~ or ~build.sh~ to compile the project.
  If you have trouble with this or later steps, try `make clean` to reset the build files.
  
  Now you should be able to run the compiler using run.sh.

  To enable optimizations you can add ~-O all~.

  For example:

  ~./run.sh --debug tests/codegen/input/01-callout.dcf -O all~

  ~./run.sh --debug tests/codegen/input/04-math2.dcf -o 04-math2.asm~

  Alternatively, you can use ~compile.sh~ to compile and run a program.
  ~./compile.sh tests/codegen/input/01-callout.dcf~
  This will assemble the program into ~tmp/out.S~, print the assembly, create an executable at ~tmp/out~,
  and run the executable, printing its output as well. This will not work for programs which need to be
  linked to external libraries, for those please use ~run.sh~ or another script.
  
* Optimizations
  Some of the optimizations below were previously described in `doc/data_flow/DataFlowAnalysis.pdf`.
  They are described here again because many described before were disabled and have since been enabled.
  
  Most optimizations use analyses from `src/scala/dataflow`.

** Interface

  To run the compiler with optimizations use the flag ~-O all~ or ~--opt=all~. This will run all implemented
  optimizations with the order specified in the Order section.
  The availabe optimizations are listed below as well as in the ~optimizations~ value in ~src/scala/Compiler.scala~.


  | Short Name    | Full Name                        | Source File                     |
  |---------------+----------------------------------+---------------------------------|
  | cse           | Common Subexpression Elimination | ~CSE~                           |
  | copyprop      | Copy Propagation                 | ~CopyPropagation~               |
  | deadcode      | Dead Code Elimination            | ~DeadCodeElimMulti~             |
  | unreachable   | Unreachable Code Elimination     | ~UnreachableCodeElim~           |
  | peep          | Algebraic Peephole               | ~PeepholeAlgebra~               |
  | inline        | Method Inlining                  | ~Inline~                        |
  | localize      | Localization                     | ~GlobalToLocal~                 |
  | regs          | Register Allocation              | ~src/scala/register_allocation~ |

  Source File in the table to a ~.scala~ file in ~src/scala/transform~. There are more transformations
  in that directory that are necessarily utilities but not optimizations.

  Multiple optimizations can also be specified with ~--opt=cse,copyprop~ but note that the order listed in the option
  will not have any effect and that the optimizations may be run more than once. If using debug mode, you can tell
  which optimizations were run, how many times, and in what order by looking at stderr or the graph files generated
  in ~tmp/~.

** Common Subexpression Elimination
    Operates on: Set of Expressions
    Available Expressions Analysis is a forward-running algorithm that determines wether an expressions variables are reassigned before the block. 
    Every assignment to x removes any expressions in the state that depend on variable x 
     
  Here is an example of CSE optimization. Please note that the blocks were given names for debugging 
  purposes, and that circular nodes are forking conditions.

  Before: 

  #+ATTR_LaTeX: width=0.4\textwidth
  [[./before_cse.png]]
  
  After CSE:

  #+ATTR_LaTeX: width=0.4\textwidth
  [[./after_cse.png]]

  Notice that CSE temps are generated for the expressions ~0~, ~a+b~, and ~c+d~, and also that ~c+d~
  appropriately is available for the last statement after it became available in each input branch. 

  TODO convince the reader that the optimization is beneficial
  TODO convince the reader that the optimization is general
  TODO convince the reader that the optimization is correct

** Unreachable Code Elimination
    Unreachable code elimination, located in ~src/scala/transform/UnreachableCodeElim.scala~, 
    deletes portions of code which can never be reached. It consumes the Reachable analysis. Any 
    block code deemed unreachable by Reachable is turned into a nop block with no statements. The 
    nop blocks are cleaned up in a later pass by the Condenser.

    Here is an example of UCE at work. Note that the blocks after the return in the if are not cleared because they could occur, but the blocks after the definitive return are cleared.

    TODO convince the reader that the optimization is beneficial
    TODO convince the reader that the optimization is general
    TODO convince the reader that the optimization is correct

** Dead Code Elimination
  If there is an assignment to a dead variable, it is first checked for whether it is a method call. 
  If it is, it is converted simply to a method call without an assignment. Otherwise, the assignment
  to a dead variable is removed completely. 

  Here is an example of before and after Dead Code Elimination runs. Please note that the blocks were
  given names for debugging purposes, and that circular nodes are forking conditions.

  Before: 

  #+ATTR_LaTeX: width=0.4\textwidth
  [[./before_deadcode_example.png]]
  
  After dead code elimination:

  #+ATTR_LaTeX: width=0.4\textwidth
  [[./after_deadcode_example.png]]

  Notice that the dead statements have been removed (the now-empty blocks will be removed in a later step).

  TODO convince the reader that the optimization is beneficial
  TODO convince the reader that the optimization is general
  TODO convince the reader that the optimization is correct

** Copy Propogation
  Copy Propagation uses the analysis of Reaching Definitions. For every use of a variable, it tries to
  replace that with the last value the variable was assigned (along every path to the current line, 
  hence reaching definitions). We have implemented a simple version that doesn't do a replacement if more
  than one path assigned a value, although it could be modified to make sure that each path assigns the same
  value, and then copy propagate. When there is a viable copy propagation (the right hand side of the reaching
  assignment is a single variable or constant), it is not done if the target value is a parameter and the 
  target asignee is a call argument, because we want to explicitly load parameters into local variables when 
  setting up a call. This is the only specially handled edge case.

  We know copy propagation is beneficial and general because it is one we learned in class. Furthermore,
  its main benefit is in causing unnecessary temps to become unused, so they can be deleted.
  TODO convince the reader that the optimization is correct

  As an example, take this sample code from ~tests/optimizer/input/noise_median.dcf~ in the method ~read~. 
  After the simplification steps that generated 13 temp variables, there was some code that looked like
  this:

  ~t0 = rows; ... i = t7; ... t6 = i < t0;~

  After running copy prop, it looked like this:
  
  ~t0 = rows; ... i = t7; ... t6 = t7 < rows;~

  At that point, t0 was not used anywhere and was able to be eliminated.

** Algebraic Peephole
  TODO describe optimization
  TODO convince the reader that the optimization is beneficial
  TODO convince the reader that the optimization is general
  TODO convince the reader that the optimization is correct

** Method Inlining
  TODO describe optimization
  TODO convince the reader that the optimization is beneficial
  TODO convince the reader that the optimization is general
  TODO convince the reader that the optimization is correct

** Global Localization

  Global Localization turns globals into locals.
  The motivation for this optimization is that many of our other optimizations remain conservative
  by not operating on gloabls. This is because globals are harder to reason about due to their
  sharing beyond the scope of the method.

  Global Localization works by examining which globals are used by which methods, identifying
  globals used by only one method, and moving the global into that methods local symbol table.

  Global Localization is not beneficial by itself, but enables other optimizations which avoid
  operating on globals to operate on those variables after they have been localized.
  Global Localization is made more powerful in some cases by Method Inlining, because inlining
  can cause a global that was used by 2 methods to be uesd by only 1 after inlining, thus GL can operate on the global.

  TODO example
  TODO convince the reader that the optimization is beneficial
  TODO convince the reader that the optimization is general
  TODO convince the reader that the optimization is correct

** Register Allocation

  When run without Register Allocation, all variables end up stored on the stack.

  TODO describe optimization
  TODO convince the reader that the optimization is beneficial
  TODO convince the reader that the optimization is general
  TODO convince the reader that the optimization is correct

** Not Implemented

  List Scheduling, Instruction Selection, Data Parallelization
  TODO explain why we did not implement these
  TODO For each optimization that you decide not to implement, you
       must convince the reader that the optimization would not be beneficial given your generated code
       for the given benchmarks and our target architecture.
       (^ that seems like not our reasons.)

** Order

  The order in which optimizations are run is critical to the operation of the compiler.
  For example, Common Subexpression Elimination generates extra variables which Copy Propogation can clean up,
  this Copy Propogation should always be run at some point after the last Common Subexpression Elimination pass.
  Another example, Dead Code Elimination should not be run first and only first because other optimizations may
  cause dead code to appear that Dead Code Elimination should have the chance to eliminate.

  Optimizations are organized into three ordered lists. A ~PRE~ list, a ~LOOP~ list, and a ~POST~ list.
  These can be seen in ~Compiler.scala~ as variables with the ~recipe~ prefix. First every pass
  in the the ~PRE~ list is run in order, then passes in ~LOOP~ are run twice in order, and then passes in ~POST~ are run.

  We have found 2 iterations of ~LOOP~ to yield a reasonable tradeoff between performance and compilation time.

  Here are the optimizations and a brief justification of their place in the organization:
*** PRE
  Optimizations first run on the raw code

  - Inlining - Opportunities for inlining are unlikely to be created by later optimizations, so just run it first so that the other optimizations are able to consider larger methods, hopefully finding more to do.
  - Global Localization - Many other optimizations to do not treat globals, so doing this first enables other optimizations to work on variables they would not otherwise.
  - Copy Propogation
  - Dead Code Elimination
  - Unreachable Code Elimination - CP, DCE, and UCE all strictly reduce the amount of code that exists, so running them initially helps to eliminate user and automatically introduced redundancy which improves the speed of other optimizations such as Common Subexpression Elimination that introduce more code.

*** LOOP
  Optimizations run repeatedly on the code

  - Common Subexpression Elimination
  - Copy Propogation - Copy Propogation and Common Subexpression Elimination have a symbiotic relationship and
                       do well to be run in a loop more than once.
  - Dead Code Elimination
  - Unreachable Code Elimination - DCE and UCE eliminate code to reduce the running time of the other optimizations,
                                   enabling more optimizations to be run in a timely manner.
  - Algebraic Peephole - Peephole optimizations can occur as a result of Copy Propogation and can produce expressions that Common Subexpression Elimination can help,
                         thus it is run in a loop to produce and consume from other optimizations.

*** POST
  Optimizations run before passing to the AsmGen

  - Copy Propogation
  - Unreachable Code Elimination
  - Dead Code Elimination - CP, UCE, DCE are the part of the loop that reduce the amount of code, spin down by running these near to last.
  - Register Allocation - Since our RA has does not modify the code at except for assigning registers, it only makes to run this last.

  The compiler is designed to be run with all optimizations turned on.
  We did not make it a priority to run the optimizations in optimal order when some of them are turned,
  so the it just does the normal ordering but skips the disabled optimizations every time through.

* Latent Issues
** Label Collisions
  TODO maybe write about label collisions

* CFG Graph Generation
    A visual representation of the cfg we generate is a very useful debugging tool. 
    Graphiz is a simple and straightforward interface that takes a definition of graph, and
    produces a svg, pdf,png, or ps file with a visualization of the graph.
    We installed the dot package to show our directed graph
    To aid in debugging, we implemented a ~grapher.scala~ that takes in a CFG,
    and creates an output .gv (same as .dot) file.

** Generating Graphs
   First, make sure you have the `graphviz` package installed on your computer, which provides the program
   `dot`. Then, you can run `graph.sh` with the code. `graph.sh` takes all the same arguments that you can
   pass in to `run.sh`, so for example you could pass in only one optimization. It will generate a graph
   before and after each optimization pass, as well as generating annotated graphs for each analysis. 
   The last pass will condense blocks into more reasonable basic blocks.

   The graphs will be output in the `tmp` folder. 

   Try running:

   `./graph.sh -O all tests/dataflow/input/cse-01.dcf`

   And check out the output graphs!
